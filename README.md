# 中文特定领域医疗 RAG 问答系统

## 项目概述

本项目基于 **Qwen-2.5-7B** 大语言模型，结合 **CMedQA2** 医疗问答数据集，设计并实现了一套医疗领域的 **检索增强生成（RAG）问答系统**。该系统通过本地知识库检索机制，为用户提供更加严谨、可靠且具有专业依据的医疗咨询与辅助建议。

- **github仓库**：https://github.com/last-priest/Medical-RAG-Qwen2.5 。

## 运行环境

- **平台** ： AutoDL (https://www.autodl.com/)
- **镜像配置**： PyTorch==2.9.1 Python==3.12.3(ubuntu22.04) CUDA==12.8
- **GPU** ： RTX 4090(24GB)

## 依赖安装

```
pip install -r requirements.txt
```

## 构建过程方法

### **数据清洗**

本项目使用 **[cMedQA2](https://github.com/zhangsheng93/cMedQA2)** 中文医疗问答数据集。该数据集包含超过 20 万条真实的医患问答对，涵盖了多种疾病、症状和治疗方案，是构建医疗领域问答系统的理想数据源。本项目采用**QA对构建知识库**的方式：将 “问题 + 正回答” 拼接为文本文档：

- 单对QA格式： "【患者提问】：question【医生回答】：answer", cMedQA2_ID_number

cMedQA2_ID_number用于 Demo 的来源追溯。

### **文本分块 (Chunking)**

- 使用 LangChain 中的**通用文本切分器**。
- **策略**: 将长文档切分为 **500 tokens** 的片段，设置 **100 tokens** 的重叠 (overlap)，以防止切分点丢失关键语义信息。

### **向量化 (Embedding)**

- 模型: 使用 `bge-m3` 模型将文本片段转换为高维向量。
- 该模型在中文语义检索任务上表现优异，能够有效捕捉医疗文本的语义特征。

### **向量数据库构建**（Storage）

- 将这些向量连同原始文本块和元数据一起存入数据库Chroma。

### 检索流程 (Retrieval)

当用户输入问题（Query）时：

1. **问题向量化**：将用户的提问用同一个模型转为向量。
2. **相似度搜索**：在数据库中计算提问向量与库中所有向量的“余弦相似度”。
3. **Top-K 返回**：返回距离最近（最相关）的前 k 个文本块(默认 3)。

## 文件作用介绍

- **process_data.py**: 专门用于处理原始的CMedQA2数据集。它负责读取原始的CSV问答对文件，进行去重、异常值过滤，并按照医疗问答的逻辑重新格式化数据，为后续的向量化做准备。
- **exp.py**: RAG系统的核心推理逻辑脚本，用于测试本地检索与生成流程。并且基于streamlit构建的Web端问答界面，支持流式输出与检索溯源展示。
- **model_download.py**：用于下载**Embedding模型**bge-m3和**大模型**Qwen-2.5-7B-Instruct到本地。
- **make_sft_data.py**：负责大模型微调的数据处理。

## 启动方式

python -m streamlit run exp.py

## 系统迭代历程

### 迭代 1：基础功能实现

- 实现了基础的检索问答逻辑。
- 后续迭代方向：计划引入 LoRA/QLoRA 技术对模型进行医疗指令微调（SFT），进一步优化模型输出的专业医患语气与科室对齐度。

### 迭代 2： 引入LoRA对大模型进行微调

- 把微调数据数据拼成 Qwen 的对话格式

### 迭代 3：生成质量优化

- 优化了Prompt模板，引入ChatML格式 (`<|im_start|>system和<|im_end|>`)。
- **解决问题**: 通过调整`temperature=0.3`和`repetition_penalty=1.2`，解决了AI回复冗余、复读以及原样复述Prompt资料内容的问题。

## 运行截图

支持多轮对话：![image-20260105221138327](image\image-20260105221138327.png)

拒绝不确定回答![image-20260105221248736](image\image-20260105221248736.png)


