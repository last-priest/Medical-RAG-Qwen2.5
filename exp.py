import streamlit as st
import os
import time

# å…³é”®ï¼šè®¾ç½® HuggingFace é•œåƒåœ°å€ (é˜²æ­¢ä¸‹è½½è¿æ¥è¶…æ—¶)
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

# å¼•å…¥ LangChain ç»„ä»¶
from langchain_community.document_loaders import CSVLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_huggingface import HuggingFaceEmbeddings


# ==========================================
# é…ç½®åŒºåŸŸ (è®°å¾—ä¿®æ”¹ API Key)
# ==========================================
# ğŸ›‘ è®°å¾—æŠŠæ ‡é¢˜æ”¹äº†ï¼
ST_TITLE = "ğŸ¥ æ™ºèƒ½åŒ»ç–—è¯Šæ–­åŠ©æ‰‹ (åŸºäº Qwen-2.5 & RAG)"

# ä½ çš„ Key (æ³¨æ„ä¿å¯†)
os.environ["OPENAI_API_KEY"] = "sk-okycixattvhctihwyrnokgeuyylxqxudrykublvsjywwvcdn" 
os.environ["OPENAI_API_BASE"] = "https://api.siliconflow.cn/v1"

MODEL_NAME = "Qwen/Qwen2.5-7B-Instruct"

# ==========================================
# æ ¸å¿ƒé€»è¾‘
# ==========================================
@st.cache_resource
def initialize_rag_system():
    # 1. åŠ è½½æ•°æ®
    if not os.path.exists("clean_medical_knowledge.csv"):
        return None, "è¯·å…ˆè¿è¡Œ process_data.py ç”Ÿæˆæ•°æ®æ–‡ä»¶ï¼"

    print("ğŸ“„ æ­£åœ¨åŠ è½½åŒ»ç–—æ•°æ®é›†...")
    loader = CSVLoader(
        file_path="./clean_medical_knowledge.csv", 
        encoding="utf-8",
        source_column="source"  # è¿™é‡ŒæŒ‡å®šäº† metadata è¯»å–å“ªä¸€åˆ—
    )
    docs = loader.load()

    # âš ï¸ è°ƒè¯•ç”¨ï¼šå¦‚æœè·‘å¾—å¤ªæ…¢ï¼Œå¯ä»¥å…ˆè§£é™¤æ³¨é‡Šä¸‹é¢è¿™è¡Œï¼Œåªå–å‰ 1000 æ¡æµ‹è¯•
    # docs = docs[:1000]

    # 2. åˆ‡åˆ†
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    splits = splitter.split_documents(docs)

    # 3. å‘é‡åŒ– (ä½¿ç”¨æœ¬åœ°æ¨¡å‹)
    print("â¬‡ï¸ æ­£åœ¨åŠ è½½æœ¬åœ° Embedding æ¨¡å‹...")
    embeddings = HuggingFaceEmbeddings(
        model_name="./models/Xorbits/bge-m3", # æŒ‡å‘ä½ ä¸‹è½½å¥½çš„è·¯å¾„
        model_kwargs={'device': 'cuda'},      # æœåŠ¡å™¨æœ‰æ˜¾å¡å°±ç”¨ cudaï¼Œæ²¡æœ‰å°± cpu
        encode_kwargs={'normalize_embeddings': True}
    )

    print("ğŸš€ æ­£åœ¨æ„å»ºå‘é‡æ•°æ®åº“ (Chroma)...")
    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)

    # 4. æ„å»ºæ£€ç´¢å™¨ (k=3 è¡¨ç¤ºæ¯æ¬¡æ‰¾ 3 æ¡æœ€ç›¸å…³çš„)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 1})

    # 5. å®šä¹‰ LLM
    llm = ChatOpenAI(
        model_name=MODEL_NAME,
        temperature=0.1, # åŒ»ç–—åœºæ™¯æ¸©åº¦è¦ä½ï¼Œä¿æŒä¸¥è°¨
        streaming=True
    )

    # 6. å®šä¹‰ Prompt (æ”¹ä¸ºåŒ»ç–—ä¸“å®¶)
    system_prompt = """
    ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„ã€ä¸‰ç”²åŒ»é™¢ä¸»æ²»åŒ»å¸ˆã€‘ã€‚è¯·åŸºäºä»¥ä¸‹ã€å‚è€ƒèµ„æ–™ã€‘å’Œã€å¯¹è¯å†å²ã€‘å›ç­”æ‚£è€…çš„é—®é¢˜ã€‚
    
    è¦æ±‚ï¼š
    1. å›ç­”å¿…é¡»åŸºäºæä¾›çš„å‚è€ƒèµ„æ–™ï¼Œä¸¥ç¦ç¼–é€ ã€‚
    2. å¦‚æœå‚è€ƒèµ„æ–™ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œè¯·ç›´æ¥å›ç­”ï¼šâ€œæŠ±æ­‰ï¼Œç›®å‰çš„åŒ»ç–—æ•°æ®åº“ä¸­æ²¡æœ‰å…³äºè¯¥é—®é¢˜çš„è®°å½•ã€‚â€
    3. è¯­æ°”è¦ä¸“ä¸šã€äº²åˆ‡ã€å¯Œæœ‰åŒç†å¿ƒã€‚

    ã€å‚è€ƒèµ„æ–™ã€‘ï¼š
    {context}
    """

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        MessagesPlaceholder(variable_name="chat_history"), # ğŸ‘ˆ è¿™é‡Œå°±æ˜¯æ¤å…¥è®°å¿†çš„å…³é”®
        ("human", "{question}")
    ])

    # 7. è¿™æ˜¯ä¸€ä¸ªçº¯ç”Ÿæˆé“¾ (Prompt + LLM)
    # æˆ‘ä»¬æŠŠæ£€ç´¢æ­¥éª¤ç§»åˆ° UI å±‚å»æ‰‹åŠ¨æ‰§è¡Œï¼Œè¿™æ ·å°±èƒ½å®Œç¾æ§åˆ¶æµå¼è¾“å‡ºäº†
    generation_chain = prompt | llm | StrOutputParser()

    # âš ï¸ ä¿®æ”¹è¿”å›å€¼ï¼šåˆ†åˆ«è¿”å› æ£€ç´¢å™¨ å’Œ ç”Ÿæˆé“¾
    return retriever, generation_chain, "ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ"


# ==========================================
# Streamlit UI ç•Œé¢é€»è¾‘
# ==========================================
st.set_page_config(page_title=ST_TITLE, page_icon="ğŸ¥", layout="wide")
st.title(ST_TITLE)

# ä¾§è¾¹æ 
with st.sidebar:
    st.header("âš™ï¸ æ§åˆ¶é¢æ¿")
    with st.spinner("æ­£åœ¨å¯åŠ¨åŒ»ç–—çŸ¥è¯†å¼•æ“..."):
        # æ¥æ”¶ä¸¤ä¸ªå¯¹è±¡
        retriever, generation_chain, msg = initialize_rag_system()
    
    if retriever and generation_chain: # åˆ¤æ–­ä¸¤ä¸ªéƒ½åœ¨
        st.success("âœ… çŸ¥è¯†åº“æŒ‚è½½æˆåŠŸ")
        st.info(f"ğŸ§  æ¨¡å‹: {MODEL_NAME}")
    else:
        st.error(f"âŒ å¯åŠ¨å¤±è´¥: {msg}")
        st.stop()

    if st.button("ğŸ§¹ æ¸…ç©ºå¯¹è¯"):
        st.session_state.messages = []
        st.rerun()

# åˆå§‹åŒ–å†å²
if "messages" not in st.session_state:
    st.session_state.messages = []

# æ˜¾ç¤ºå†å²
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        # å¦‚æœå†å²æ¶ˆæ¯é‡Œæœ‰å¼•ç”¨æ¥æºï¼Œä¹Ÿæ˜¾ç¤ºå‡ºæ¥
        if "sources" in message:
            with st.expander("ğŸ“š å‚è€ƒæ¥æº (Citation)"):
                st.markdown(message["sources"])

# å¤„ç†è¾“å…¥
# ==========================================
# æ ¸å¿ƒäº¤äº’åŒºåŸŸ (åŒ…å«æ‰“å­—æœºæ•ˆæœ + å¼•ç”¨æ˜¾ç¤º)
# ==========================================
if prompt := st.chat_input("è¯·æè¿°æ‚¨çš„ç—‡çŠ¶æˆ–é—®é¢˜..."):
    # 1. æ˜¾ç¤ºç”¨æˆ·è¾“å…¥
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.chat_message("assistant"):
        # ------------------------------------------
        # æ­¥éª¤ 1: æ£€ç´¢ (Retrieval) - æ‰¾èµ„æ–™
        # ------------------------------------------
        status_placeholder = st.empty()
        status_placeholder.markdown("ğŸ” æ­£åœ¨æ£€ç´¢åŒ»ç–—æ•°æ®åº“...")
        
        # æ‰‹åŠ¨æ‰§è¡Œæ£€ç´¢
        docs = retriever.invoke(prompt)
        
        # å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹æ‹¼æ¥æˆå­—ç¬¦ä¸²
        context_text = "\n\n".join([doc.page_content for doc in docs])
        
        # æ£€ç´¢å®Œæˆåéšè—æç¤º
        status_placeholder.empty()

        # ------------------------------------------
        # æ­¥éª¤ 2: ç”Ÿæˆ (Generation) - æµå¼æ‰“å­—æœº
        # ------------------------------------------
        response_placeholder = st.empty()
        full_response = ""
        
        # ä½¿ç”¨ .stream() å¯ç”¨æµå¼è¾“å‡º
        # æˆ‘ä»¬æŠŠåˆšæ‰æ£€ç´¢åˆ°çš„ context_text æ‰‹åŠ¨ä¼ ç»™é“¾
        try:
            # âœ… æ–°å¢ä»£ç å¼€å§‹ï¼šæ„å»ºå†å²è®°å½•å¯¹è±¡ -------------
            history_buffer = []
            # éå†å†å²è®°å½• (æ’é™¤æ‰æœ€æ–°çš„ä¸€æ¡ç”¨æˆ·æé—®ï¼Œå› ä¸ºé‚£ä¸ªä¼šé€šè¿‡ question å‚æ•°ä¼ å…¥)
            # æ³¨æ„ï¼šst.session_state.messages æ­¤æ—¶å·²ç»åŒ…å«äº†æœ€æ–°çš„ promptï¼Œæ‰€ä»¥æˆ‘ä»¬è¦ [:-1]
            for msg in st.session_state.messages[:-1]:
                if msg["role"] == "user":
                    history_buffer.append(HumanMessage(content=msg["content"]))
                elif msg["role"] == "assistant":
                    history_buffer.append(AIMessage(content=msg["content"]))
            # âœ… æ–°å¢ä»£ç ç»“æŸ -----------------------------

            # ä¿®æ”¹ .stream() çš„è°ƒç”¨å‚æ•°
            stream = generation_chain.stream({
                "context": context_text, 
                "question": prompt,
                "chat_history": history_buffer # ğŸ‘ˆ æŠŠè½¬æ¢å¥½çš„å†å²ä¼ è¿›å»
            })

            for chunk in stream:
                full_response += chunk
                # â–Œ æ˜¯å…‰æ ‡æ•ˆæœï¼Œæ¨¡æ‹Ÿæ‰“å­—
                response_placeholder.markdown(full_response + "â–Œ")
                # å¦‚æœæœ¬åœ°è·‘å¤ªå¿«çœ‹ä¸æ¸…ï¼Œå¯ä»¥å–æ¶ˆä¸‹é¢è¿™è¡Œçš„æ³¨é‡Š
                time.sleep(0.02) 

            # å¾ªç¯ç»“æŸï¼ŒæŠŠå…‰æ ‡å»æ‰ï¼Œæ˜¾ç¤ºæœ€ç»ˆå®Œæ•´ç»“æœ
            response_placeholder.markdown(full_response)

        except Exception as e:
            st.error(f"ç”Ÿæˆæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            full_response = "æŠ±æ­‰ï¼Œç³»ç»Ÿç”Ÿæˆå›ç­”æ—¶é‡åˆ°æ•…éšœã€‚"

        # ------------------------------------------
        # æ­¥éª¤ 3: å¤„ç†å¼•ç”¨æ¥æº (Citation)
        # ------------------------------------------
        source_text = ""
        unique_sources = set()
        for doc in docs:
            # è·å– metadata é‡Œçš„ source
            src = doc.metadata.get('source', 'æœªçŸ¥æ¥æº')
            if src not in unique_sources:
                unique_sources.add(src)
                # å¯ä»¥åœ¨è¿™é‡Œåšä¸€äº›ç¾åŒ–ï¼Œæ¯”å¦‚æŠŠ source ID å˜æˆæ–‡ä»¶å
                source_text += f"- ğŸ“„ **è¯æ®æ¥æº**: `{src}`\n"

        if source_text:
            with st.expander("ğŸ“š å‚è€ƒæ¥æº (Citation)"):
                st.markdown(source_text)

    # 4. å­˜å…¥å†å² (åŒ…å«å¼•ç”¨ä¿¡æ¯)
    st.session_state.messages.append({
        "role": "assistant", 
        "content": full_response,
        "sources": source_text # é¢å¤–å­˜ä¸€ä¸ªå­—æ®µ
    })