import json
import pandas as pd
import time
import re
import os
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from exp import initialize_rag_system

# ================= é…ç½®åŒºåŸŸ =================
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["OPENAI_API_KEY"] = "sk-okycixattvhctihwyrnokgeuyylxqxudrykublvsjywwvcdn" 
os.environ["OPENAI_API_BASE"] = "https://api.siliconflow.cn/v1"
# ===========================================

# ğŸ”¥ æ ¸å¿ƒå‡çº§ï¼šå¤šç»´åº¦è¯„åˆ† Prompt
# æˆ‘ä»¬è¦æ±‚ AI ä»ä¸‰ä¸ªç‹¬ç«‹ç»´åº¦æ‰“åˆ†ï¼Œå¹¶è¾“å‡º JSON
# ğŸ”¥ æ ¸å¿ƒå‡çº§ï¼šCoT (æ€ç»´é“¾) è¯„åˆ†æ¨¡æ¿
ADVANCED_EVAL_TEMPLATE = """
ä½ æ˜¯ä¸€ä½æå…¶ä¸¥æ ¼çš„ NLP è¯„ä¼°ä¸“å®¶ã€‚è¯·åŸºäºå‚è€ƒèµ„æ–™å’Œæ ‡å‡†ç­”æ¡ˆï¼Œå¯¹è€ƒç”Ÿå›ç­”è¿›è¡Œâ€œæ‰¾èŒ¬â€å¼è¯„åˆ†ã€‚

ã€å‚è€ƒèµ„æ–™ (Context)ã€‘ï¼š
{context}

ã€æ ‡å‡†ç­”æ¡ˆ (Ground Truth)ã€‘ï¼š
{ground_truth}

ã€è€ƒç”Ÿå›ç­” (Answer)ã€‘ï¼š
{answer}

---
è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ€è€ƒï¼ˆä¸è¦è·³è¿‡ï¼ï¼‰ï¼š
1. æ£€æŸ¥ **å‡†ç¡®æ€§**ï¼šè€ƒç”Ÿå›ç­”æ˜¯å¦é—æ¼äº†æ ‡å‡†ç­”æ¡ˆé‡Œçš„å…³é”®ç‚¹ï¼Ÿ(é—æ¼äº†å°±æ‰£åˆ†)
2. æ£€æŸ¥ **å¿ å®åº¦**ï¼šè€ƒç”Ÿå›ç­”é‡Œæœ‰æ²¡æœ‰å‚è€ƒèµ„æ–™é‡Œæ²¡æåˆ°çš„åºŸè¯ï¼Ÿ(æœ‰åºŸè¯å¿…é¡»æ‰£åˆ†ï¼Œå“ªæ€•æ˜¯å¯¹çš„ä¹Ÿè¦æ‰£ï¼)
3. æ£€æŸ¥ **å¼•ç”¨**ï¼šè€ƒç”Ÿæ˜¯å¦å……åˆ†åˆ©ç”¨äº†èµ„æ–™ï¼Ÿ

æœ€åè¾“å‡º JSONã€‚

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼ˆåˆ†æ•°å¿…é¡»æ˜¯ 0.0, 0.3, 0.5, 0.8, 1.0 ä¸­çš„ä¸€ä¸ªï¼Œä»¥æ­¤æ‹‰å¼€å·®è·ï¼‰ï¼š
{{
    "reasoning": "ç®€çŸ­çš„ä¸€å¥è¯ï¼ŒæŒ‡å‡ºå…·ä½“å“ªé‡Œæ‰£åˆ†äº†",
    "accuracy": 0.x,
    "faithfulness": 0.x,
    "citation_f1": 0.x
}}
"""

def advanced_evaluate():
    print("ğŸš€ åˆå§‹åŒ– RAG ç³»ç»Ÿ (é«˜çº§æ¨¡å¼)...")
    retriever, generation_chain, _ = initialize_rag_system()
    
    # âš ï¸ ä½¿ç”¨ temperature=0.0ï¼Œè®©æ¨¡å‹è¾“å‡º JSON æ›´ç¨³å®š
    evaluator_llm = ChatOpenAI(model="Qwen/Qwen2.5-7B-Instruct", temperature=0.0)
    eval_chain = ChatPromptTemplate.from_template(ADVANCED_EVAL_TEMPLATE) | evaluator_llm | StrOutputParser()

    print("ğŸ“‚ è¯»å–æµ‹è¯•é›† test_dataset.json ...")
    with open('test_dataset.json', 'r', encoding='utf-8') as f:
        test_data = json.load(f)

    # âš ï¸ ä¿®æ”¹æ–‡ä»¶åï¼Œé¿å…å’Œä¹‹å‰çš„æ··æ·†
    output_file = "advanced_evaluation.xlsx"
    results = []
    
    # === æ–­ç‚¹ç»­ä¼ é€»è¾‘ ===
    if os.path.exists(output_file):
        print("ğŸ”„ æ£€æµ‹åˆ°ä¸Šæ¬¡è¿è¡Œçš„è®°å½•ï¼Œæ­£åœ¨å°è¯•è¯»å–...")
        try:
            existing_df = pd.read_excel(output_file)
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åŒ…å«æ–°æŒ‡æ ‡åˆ—ï¼Œå¦‚æœæ˜¯æ—§æ ¼å¼åˆ™ä¸è¯»å–
            if 'accuracy' in existing_df.columns:
                results = existing_df.to_dict('records')
                print(f"âœ… å·²è·³è¿‡å‰ {len(results)} ä¸ªå·²å®Œæˆçš„é—®é¢˜")
            else:
                print("âš ï¸ æ£€æµ‹åˆ°æ—§æ ¼å¼è¡¨æ ¼ï¼Œå°†é‡æ–°å¼€å§‹ç”Ÿæˆ...")
                results = []
        except:
            print("âš ï¸ è¯»å–å¤±è´¥ï¼Œé‡æ–°å¼€å§‹")
            results = []

    print(f"âš¡ å¼€å§‹ç¡¬æ ¸è¯„ä¼° (å…± {len(test_data)} é¢˜)...")
    
    start_index = len(results)
    
    for i in range(start_index, len(test_data)):
        item = test_data[i]
        q = item['question']
        gt = item['ground_truth']
        
        print(f"\n-------- ç¬¬ {i+1}/{len(test_data)} é¢˜ --------")
        print(f"â“ é—®é¢˜: {q}")
        
        try:
            # 1. æ£€ç´¢ (Context éå¸¸é‡è¦ï¼)
            docs = retriever.invoke(q)
            # ç»™æ–‡æ¡£åŠ ä¸ªåºå·ï¼Œæ–¹ä¾¿ LLM è¯†åˆ«
            context_text = "\n".join([f"[{j+1}] {d.page_content}" for j, d in enumerate(docs)])
            
            # 2. ç”Ÿæˆå›ç­”
            print("ğŸ¤– æ­£åœ¨ç”Ÿæˆå›ç­”...")
            response = generation_chain.invoke({
            "context": context_text, 
            "question": q,
            "chat_history": [] 
            })
            print(f"ğŸ’¬ å›ç­”é¢„è§ˆ: {response[:20]}...")
            
            # ğŸ›‘ ä¼‘æ¯ 20 ç§’ (ä¿æŒä½ çš„å®‰å…¨è®¾ç½®)
            print("â³ ç”Ÿæˆå®Œæ¯•ï¼Œä¼‘æ¯ 20 ç§’...")
            time.sleep(20) 
            
            # 3. LLM ä¸‰ç»´åˆ¤å·
            print("ğŸ‘¨â€ğŸ« æ­£åœ¨è¿›è¡Œå¤šç»´åº¦è¯„åˆ†...")
            eval_result_str = eval_chain.invoke({
                "context": context_text,
                "ground_truth": gt,
                "answer": response
            })
            
            # 4. è§£æ JSON (æ¸…æ´—æ•°æ®)
            # æœ‰æ—¶å€™æ¨¡å‹ä¼šåŠ  ```json ... ```ï¼Œéœ€è¦å»æ‰
            clean_json = eval_result_str.replace("```json", "").replace("```", "").strip()
            
            try:
                scores = json.loads(clean_json)
            except json.JSONDecodeError:
                # ä¸‡ä¸€è§£æå¤±è´¥ï¼Œç»™ä¸ªä¿åº•åˆ†ï¼Œå¹¶è®°å½•é”™è¯¯
                print(f"âš ï¸ JSON è§£æå¤±è´¥ï¼ŒåŸå§‹è¿”å›: {clean_json}")
                scores = {"accuracy": 0.5, "faithfulness": 0.5, "citation_f1": 0.5, "reason": "è§£æå¤±è´¥"}
            
            print(f"ğŸ“Š ç»“æœ: accuracy{scores.get('accuracy')} / faithfulness{scores.get('faithfulness')} / citation_f1{scores.get('citation_f1')}")
            
            # 5. å­˜å…¥ç»“æœ
            results.append({
                "question": q,
                "ground_truth": gt,
                "answer": response,
                "contexts": context_text, # æŠŠå‚è€ƒèµ„æ–™ä¹Ÿå­˜ä¸‹æ¥ï¼Œæ˜¾å¾—ä¸“ä¸š
                "accuracy": scores.get('accuracy', 0),
                "citation_f1": scores.get('citation_f1', 0),
                "faithfulness": scores.get('faithfulness', 0),
                # è‡ªåŠ¨è®¡ç®—å¹»è§‰ç‡
                "hallucination_rate": 1.0 - float(scores.get('faithfulness', 0)), 
                "reason": scores.get('reason', '')
            })
            
            # ğŸ’¾ å®æ—¶ä¿å­˜
            pd.DataFrame(results).to_excel(output_file, index=False)
            print("ğŸ’¾ è¿›åº¦å·²ä¿å­˜")
            
            # ğŸ›‘ å†æ¬¡ä¼‘æ¯ 20 ç§’
            print("â³ è¯„åˆ†å®Œæ¯•ï¼Œä¼‘æ¯ 20 ç§’...")
            time.sleep(20)

        except Exception as e:
            print(f"âŒ æœ¬é¢˜å‡ºé”™: {e}")
            # å‡ºé”™åçš„é•¿ä¼‘æ¯
            time.sleep(60)

    print(f"\nğŸ‰ ç¡¬æ ¸è¯„ä¼°å®Œæˆï¼è¯·æŸ¥çœ‹ {output_file}")

if __name__ == "__main__":
    advanced_evaluate()